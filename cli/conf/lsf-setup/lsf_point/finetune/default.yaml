hydra:
  run:
    dir: outputs/lsf-setup/lsf_point/finetune/${hydra:runtime.choices.model}/${exp_name}/${model.finetune_pattern}/${hydra:runtime.choices.data}/${run_name}
defaults:
  - model: ???
  - data: ???
  - val_data: null
  - _self_
exp_name: ???
run_name: ???
seed: 0
tf32: true
compile: false  # set to mode: default, reduce-overhead, max-autotune
ckpt_path: null
trainer:
  _target_: lightning.Trainer
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: 32
  logger:
      _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ${hydra:runtime.output_dir}
      name: logs
  callbacks:
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}/checkpoints
      monitor: val/MSE # val/PackedNLLLoss
      save_weights_only: true
      mode: min
      save_top_k: 1  # Qz: Sometimes the 1st validation gets anomalous results. Discard that ckpt, and use the 2nd one.
      every_n_epochs: 1
    - _target_: lightning.pytorch.callbacks.EarlyStopping # uni2ts.callbacks.earlystop.WarmupEarlyStopping
      monitor: val/MSE # val/PackedNLLLoss
      min_delta: 0.0
      patience: 10  # Set to a small value as now each epoch has many batches.
      mode: min
      strict: false
      verbose: true
#      warmup_steps: 1
  max_epochs: 1000
  enable_progress_bar: true
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
train_dataloader:
  _target_: uni2ts.data.loader.DataLoader
  batch_size: 1024 # Can use a large batch size after disabling sequence packing.
  batch_size_factor: 2.0
  cycle: false  # Set it as false to loop over all batches per epoch
  num_batches_per_epoch: null
  shuffle: true
  num_workers: 11
  pin_memory: true
  drop_last: false
  fill_last: false
  worker_init_fn: null
  prefetch_factor: 2
  persistent_workers: true
val_dataloader:
  _target_: uni2ts.data.loader.DataLoader
  batch_size: 32
  batch_size_factor: 2.0
  cycle: false
  num_batches_per_epoch: null
  shuffle: false
  num_workers: 11
  pin_memory: false
  drop_last: false
  fill_last: false
  worker_init_fn: null
  prefetch_factor: 2
  persistent_workers: true